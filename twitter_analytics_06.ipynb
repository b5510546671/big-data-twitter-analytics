{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Twitter Data\n",
    "## Data Visualisation Basics\n",
    "A picture is worth a thousand tweets: more often than not, designing a good visual representation of our data, can help us make sense of them and highlight interesting insights. After collecting and analysing Twitter data, we will now continue with some notions on data visualisation with Python.\n",
    "\n",
    "### From Python to Javascript with Vincent\n",
    "While there are some options to create plots in Python using libraries like matplotlib or ggplot, one of the coolest libraries for data visualisation is probably **D3.js** which is, as the name suggests, based on Javascript. D3 plays well with web standards like CSS and SVG, and allows to create some wonderful interactive visualisations.\n",
    "\n",
    "*Vincent* bridges the gap between a Python back-end and a front-end that supports D3.js visualisation, allowing us to benefit from both sides. The tagline of Vincent is in fact “*The data capabilities of Python. The visualization capabilities of JavaScript*”. Vincent, a Python library, takes our data in Python format and translates them into *Vega*, a JSON-based visualisation grammar that will be used on top of D3. It sounds quite complicated, but it’s fairly simple and pythonic. You don’t have to write a line in Javascript/D3 if you don’t want to.\n",
    "\n",
    "Firstly, let’s install Vincent:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install vincent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, let’s create our first plot. Using the list of most frequent terms (without hashtags) from our provious data set, we want to plot their frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from the old code\n",
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "fname = \"tweets.json\"\n",
    "with open(fname, 'r') as f:\n",
    "    count_terms_only = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # substitute, e.g. \\u2026, with space\n",
    "        tw = re.sub(r'[^\\x00-\\x7F]+',' ', tweet['text'])\n",
    "        # Count terms only (no hashtags, no mentions)\n",
    "        terms_only = [term for term in preprocess(tw)\\\n",
    "              if term not in stop and\\\n",
    "              not term.startswith(('#', '@'))]         \n",
    "        # Update the counter\n",
    "        count_terms_only.update(terms_only)\n",
    "    # Print the first 10 most frequent words\n",
    "    #print(count_terms_only.most_common(10))\n",
    "\n",
    "# new code begins here\n",
    "import vincent\n",
    " \n",
    "word_freq = count_terms_only.most_common(20)\n",
    "labels, freq = zip(*word_freq)\n",
    "data = {'data': freq, 'x': labels}\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "bar.to_json('term_freq.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the file term_freq.json will contain a description of the plot that can be handed over to D3.js and Vega. A simple template (taken from Vincent resources) to visualise the plot:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<html>  \n",
    "<head>    \n",
    "    <title>Vega Scaffold</title>\n",
    "    <script src=\"http://d3js.org/d3.v3.min.js\" charset=\"utf-8\"></script>\n",
    "    <script src=\"http://d3js.org/topojson.v1.min.js\"></script>\n",
    "    <script src=\"http://d3js.org/d3.geo.projection.v0.min.js\" charset=\"utf-8\"></script>\n",
    "    <script src=\"http://trifacta.github.com/vega/vega.js\"></script>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"vis\"></div>\n",
    "</body>\n",
    "<script type=\"text/javascript\">\n",
    "// parse a spec and create a visualization view\n",
    "function parse(spec) {\n",
    "  vg.parse.spec(spec, function(chart) { chart({el:\"#vis\"}).update(); });\n",
    "}\n",
    "parse(\"term_freq.json\");\n",
    "</script>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the above HTML page as **chart.html** and run the simple Python web server:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m SimpleHTTPServer 8889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can open your browser at http://localhost:8889/chart.html and observe the result.\n",
    "\n",
    "Notice that you could save the HTML template directly from Python with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar.to_json('term_freq.json', html_out=True, html_path='chart.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Visualisation\n",
    "\n",
    "Another interesting aspect of analysing data from Twitter is the possibility to observe the distribution of tweets over time. In other words, if we organise the frequencies into temporal buckets, we could observe how Twitter users react to real-time events.\n",
    "\n",
    "One of our favourite tools for data analysis with Python is Pandas, which also has a fairly decent support for time series. As an example, let’s track the hashtag #bigdata to observe what happened during the first match.\n",
    "\n",
    "In the main loop which reads all the tweets, we simply track the occurrences of the hashtag, i.e. we can refactor the code from the previous episodes into something similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "import sys\n",
    " \n",
    "dates_BIGDATA = []\n",
    "\n",
    "fname = \"c:\\\\Program Files\\\\Anaconda2\\\\tweets_bigData_dataAnalytic.json\"\n",
    "f = open(fname, 'r')\n",
    "# f is the file pointer to the JSON data set\n",
    "count = 1\n",
    "for line in f:\n",
    "    count = count + 1\n",
    "    if count%500 == 0:\n",
    "        sys.stdout.write('.')\n",
    "    if count%35000 == 0:\n",
    "        sys.stdout.write('\\n')\n",
    "    tweet = json.loads(line)\n",
    "    # substitute, e.g. \\u2026, with space\n",
    "    tw = re.sub(r'[^\\x00-\\x7F]+',' ', tweet['text'])\n",
    "    # let's focus on hashtags only at the moment\n",
    "    terms_hash = [term for term in preprocess(tw) if term.startswith('#')]\n",
    "    # track when the hashtag is mentioned\n",
    "    if '#bigdata' in terms_hash:\n",
    "        dates_BIGDATA.append(tweet['created_at'])\n",
    "print '\\nDone..'\n",
    " \n",
    "# a list of \"1\" to count the hashtags\n",
    "ones = [1]*len(dates_BIGDATA)\n",
    "# the index of the series\n",
    "idx = pandas.DatetimeIndex(dates_BIGDATA)\n",
    "# the actual series (at series of 1s for the moment)\n",
    "BIGDATA = pandas.Series(ones, index=idx)\n",
    " \n",
    "# Resampling / bucketing\n",
    "per_10minute = BIGDATA.resample('10Min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line is what allows us to track the frequencies over time. The series is re-sampled with intervals of 10 minutes. This means all the tweets falling within a particular minute will be aggregated, more precisely they will be summed up. The time index will not keep track of the seconds anymore. If there is no tweet in a particular minute, the *fillna()* function will fill the blanks with zeros.\n",
    "\n",
    "To put the time series, run these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "per_10minute.plot()\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than just observing one sequence at a time, we could compare different series to observe how the matches has evolved. \n",
    "\n",
    "So let’s refactor the code for the time series, keeping track of the three different hashtags #bigdata, #datascience and #python (<-- your turn to modify the code) into the corresponding pandas.Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "import sys\n",
    " \n",
    "dates_BIGDATA = []\n",
    "dates_DATASCIENCE = []\n",
    "\n",
    "fname = \"c:\\\\Program Files\\\\Anaconda2\\\\tweets_bigData_dataAnalytic.json\"\n",
    "f = open(fname, 'r')\n",
    "# f is the file pointer to the JSON data set\n",
    "count = 1\n",
    "for line in f:\n",
    "    count = count + 1\n",
    "    if count%500 == 0:\n",
    "        sys.stdout.write('.')\n",
    "    if count%35000 == 0:\n",
    "        sys.stdout.write('\\n')\n",
    "    tweet = json.loads(line)\n",
    "    # substitute, e.g. \\u2026, with space\n",
    "    tw = re.sub(r'[^\\x00-\\x7F]+',' ', tweet['text'])\n",
    "    # let's focus on hashtags only at the moment\n",
    "    terms_hash = [term for term in preprocess(tw) if term.startswith('#')]\n",
    "    # track when the hashtag is mentioned\n",
    "    if '#bigdata' in terms_hash:\n",
    "        dates_BIGDATA.append(tweet['created_at'])\n",
    "    if '#datascience' in terms_hash:\n",
    "        dates_DATASCIENCE.append(tweet['created_at'])\n",
    "\n",
    "print '\\nwait..'\n",
    "# a list of \"1\" to count the hashtags\n",
    "ones = [1]*len(dates_BIGDATA)\n",
    "# the index of the series\n",
    "idx = pandas.DatetimeIndex(dates_BIGDATA)\n",
    "# the actual series (at series of 1s for the moment)\n",
    "BIGDATA = pandas.Series(ones, index=idx)\n",
    "# Resampling / bucketing\n",
    "bd_per_minute = BIGDATA.resample('1Min').sum().fillna(0)\n",
    "\n",
    "ones = [1]*len(dates_DATASCIENCE)\n",
    "# the index of the series\n",
    "idx = pandas.DatetimeIndex(dates_DATASCIENCE)\n",
    "# the actual series (at series of 1s for the moment)\n",
    "DATASCIENCE = pandas.Series(ones, index=idx)\n",
    "# Resampling / bucketing\n",
    "ds_per_minute = DATASCIENCE.resample('1Min').sum().fillna(0)\n",
    "print 'Done..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all the data together\n",
    "match_data = dict(BigData=bd_per_minute, DataScience=ds_per_minute)\n",
    "# we need a DataFrame, to accommodate multiple series\n",
    "all_matches = pandas.DataFrame(data=match_data,\n",
    "                               index=bd_per_minute.index)\n",
    "# Resampling as above\n",
    "all_matches = all_matches.resample('10Min').sum().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "all_matches.plot()\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your turn now\n",
    "## Rather than obeserve the hastags, \n",
    "## modify the code to observe the co-occurrences,\n",
    "## such as 'big data', 'data science', ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
