{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Twitter Data\n",
    "## Tweets Processing with NLTK Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize a Tweet Text\n",
    "Now, let’s see an example, using the popular *NLTK* library to tokenise a fictitious tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "tweet = 'RT @toto How #BigData and CRM are Shaping Modern Marketing :D https://t.co/TgUYSUp9jT'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some peculiarities that are not captured by a general-purpose English tokeniser like the one from NLTK:\n",
    "\n",
    "*@-mentions*, *emoticons*, *URLs* and *#hash-tags* are not recognised as \"single tokens\". \n",
    "\n",
    "The following code will propose a pre-processing chain that will consider these aspects of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str, # the emoticon strings defined above\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = 'RT @toto How #BigData and CRM are Shaping Modern Marketing :D https://t.co/TgUYSUp9jT'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Terms\n",
    "Assuming we have already collected a list of tweets, now we will do a simple word count. \n",
    "\n",
    "In this way, we can observe what are the terms most commonly used in the data set. In this example, we’ll use the set of  tweets, so *the most frequent words should correspond to the topics we discuss* (not necessarily, but bear with be for a couple of paragraphs).\n",
    "\n",
    "We can use a custom tokeniser to split the tweets into a list of terms. The following code uses the preprocess() function described previously, in order to capture Twitter-specific aspects of the text, such as #hashtags, @-mentions, emoticons and URLs. \n",
    "\n",
    "In order to keep track of the frequencies while we are processing the tweets, we can use *collections.Counter()* which internally is a dictionary (term: count) with some useful methods like *most_common()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "fname = 'tweets.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "    # Print the first 10 most frequent words\n",
    "    most10_freqWords = count_all.most_common(10)\n",
    "    print(most10_freqWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the most frequent words (or should we say, tokens), are not exactly meaningful. ;D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop-words\n",
    "In every language, some words are particularly common. While their use in the language is crucial, they don’t usually convey a particular meaning, especially if taken out of context. This is the case of articles, conjunctions, some adverbs, etc. which are commonly called \"*stop-words*\". In the example above, we can see three common stop-words, for example '*to*', '*the*' and '*\\u2026*'. \n",
    "\n",
    "Stop-word removal is one important step that should be considered during the pre-processing stages. One can build a custom list of stop-words, or use available lists (e.g. NLTK provides a simple list for English stop-words).\n",
    "\n",
    "Given the nature of our data and our tokenisation, we should also be careful with all the punctuation marks and with terms like \"RT\" (used for re-tweets) and \"via\" (used to mention the original author of an article or a re-tweet), which are not in the default stop-word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now substitute the variable **terms_all** in the first example with something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "fname = 'tweets.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        #terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        # Update the counter\n",
    "        #count_all.update(terms_all)\n",
    "        count_all.update(terms_stop)\n",
    "    # Print the first 10 most frequent words\n",
    "    print(count_all.most_common(10))\n",
    "    # compare with the previous one\n",
    "    print \"Old ones: \" + str(most10_freqWords)\n",
    "    # note that you can include those \\u2026, de, la, ... to stopword list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More term filters\n",
    "Besides stop-word removal, we can further customise the list of terms / tokens we are interested in. Here you have some examples that you can embed in the first fragment of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "fname = 'tweets.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all_single = Counter()\n",
    "    count_all_hash = Counter()\n",
    "    count_all_only = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms, excluding stopwords\n",
    "        terms_all = [term for term in preprocess(tweet['text'])\\\n",
    "              if term not in stop]\n",
    "\n",
    "        # Count terms only once, equivalent to \"Document Frequency\"\n",
    "        terms_single = set(terms_all)\n",
    "\n",
    "        # Count hashtags only\n",
    "        terms_hash = [term for term in preprocess(tweet['text'])\\\n",
    "              if term.startswith('#')]\n",
    "\n",
    "        # Count terms only (no hashtags, no mentions)\n",
    "        terms_only = [term for term in preprocess(tweet['text'])\\\n",
    "              if term not in stop and\\\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs\n",
    "        \n",
    "        # Update the counter\n",
    "        count_all_single.update(terms_single)\n",
    "        count_all_hash.update(terms_hash)\n",
    "        count_all_only.update(terms_only)\n",
    "    # Print the first 10 most frequent words\n",
    "    print \"DF (all terms): \" + str(count_all_single.most_common(10))\n",
    "    print \"Hashtag: \" +str(count_all_hash.most_common(10))\n",
    "    print \"Term only: \" +str(count_all_only.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tuples of adjacent tokens\n",
    "While the other frequent terms represent a clear topic, more often than not simple term frequencies don’t give us a deep explanation of what the text is about. To put things in context, let’s consider sequences of two terms (a.k.a. *bigrams*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams \n",
    " \n",
    "terms_bigram = bigrams(terms_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *bigrams()* function from *NLTK* will take a list of tokens and produce a list of tuples using adjacent tokens. \n",
    "\n",
    "Notice that we could use terms_all to compute the bigrams, but we would probably end up with a lot of garbage. In case we decide to analyse longer n-grams (sequences of n tokens), it could make sense to keep the stop-words, just in case we want to capture phrases like “to be or not to be”.\n",
    "\n",
    "So after counting and sorting the bigrams, this is the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from nltk import bigrams \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "fname = 'tweets.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        terms_bigram = bigrams(terms_stop)\n",
    "        # Update the counter\n",
    "        count_all.update(terms_bigram)\n",
    "    # Print the first 10 most frequent words\n",
    "    print(count_all.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}